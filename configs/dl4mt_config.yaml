data_configs:
  lang_pair: "zh-en"
  train_data:
    - "/home/user_data/weihr/NMT_DATA_PY3/1.34M/train/zh.under50.txt"
    - "/home/user_data/weihr/NMT_DATA_PY3/1.34M/train/en.under50.txt"
  valid_data:
    - "/home/user_data/weihr/NMT_DATA_PY3/1.34M/test/MT03/zh.0"
    - "/home/user_data/weihr/NMT_DATA_PY3/1.34M/test/MT03/en.0"
  bleu_valid_reference: "/home/user_data/weihr/NMT_DATA_PY3/1.34M/test/MT03/en."
  dictionaries:
    - "/home/weihr/NMT_DATA_PY3/1.34M/dict/dict.zh.json"
    - "/home/weihr/NMT_DATA_PY3/1.34M/dict/dict.en.json"
  bpe_codes:
    - ""
    -
  use_char:
    - false
    - false
  n_words:
    - 30000
    - 30000
  max_len:
    - -1
    - -1
  num_refs: 4
  eval_at_char_level: false

model_configs:
  model: DL4MT
  d_word_vec: 512
  d_model: 1024
  dropout: 0.5
  proj_share_weight: false

optimizer_configs:
  optimizer: "adam"
  learning_rate: 0.0005
  grad_clip: 5.0
  optimizer_params: ~
  schedule_method: loss # ["loss" | "noam" | ~], see details in ./lr_schedule_examples
  scheduler_configs:
    warmup_steps: 8000
    decay_scale: 0.5
    min_lr: 0.00005
    max_patience: 20
    schedule_freq: 100

training_configs:
  max_epochs: 1000000
  shuffle: true
  use_bucket: true # Whether to use bucket. If true, model will run faster while a little bit performance regression.
  buffer_size: 1000 # Only valid when use_bucket is true.
  shard_size: -1 # Shard size. Negative value if closed.
  batch_size: 50
  valid_batch_size: 50
  bleu_valid_batch_size: 10
  bleu_valid_warmup: 10000 # Start to do BLEU validation after those steps
  bleu_valid_configs:
    bleu_script: "multi-bleu"
    lowercase: true
    postprocess: false
  disp_freq: 100 # Frequency to print information
  save_freq: 1000 # Frequency to save the model
  loss_valid_freq: &decay_freq 100
  bleu_valid_freq: 1000
  early_stop_patience: 20